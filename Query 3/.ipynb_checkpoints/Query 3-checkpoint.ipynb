{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6126fbfe-abc6-4e38-a256-983e23d44082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>663</td><td>application_1761923966900_0675</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0675/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-11.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0675_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>665</td><td>application_1761923966900_0677</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0677/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-133.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0677_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bc5b86-2037-417a-927b-270270e4f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>666</td><td>application_1761923966900_0678</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0678/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-85.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0678_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea182480e1ba4cc3b1d432fa392ec875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc91048974d4f50868110cfc71ad08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, to_date, year, count, desc, rank, sum as _sum, round, split, explode, length, trim\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 3 execution\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94789cad-a6d6-4e1d-a2c6-dbf5c0721cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac01b1ff41e415e9abfb8fb21fe2f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing the data\n",
    "Crime_data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "Recent_crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "                                      header = True, \\\n",
    "                                      schema = Crime_data_schema)\n",
    "Older_crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "                                     header = True, \\\n",
    "                                     schema = Crime_data_schema)\n",
    "Crime_df = Recent_crime_data_df.union(Older_crime_data_df)\n",
    "raw_mo = spark.read.text(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "# Here we make a list of two elements: the MO code as the first element and\n",
    "# the MO description as the second\n",
    "MO_codes_df = raw_mo.select(\n",
    "    split(col(\"value\"), \" \", 2)[0].alias(\"MO_code\"),\n",
    "    split(col(\"value\"), \" \", 2)[1].alias(\"MO_desc\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c26e75-419a-4ef0-aee9-46ad1afa9dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7aa8a8ce3b343bc9d26d18a82a95431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Strategy: BROADCAST ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [MO_code#159], [MO_code#153], Inner, BuildRight, false\n",
      "   :- HashAggregate(keys=[MO_code#159], functions=[count(1)], schema specialized)\n",
      "   :  +- Exchange hashpartitioning(MO_code#159, 1000), ENSURE_REQUIREMENTS, [plan_id=57]\n",
      "   :     +- HashAggregate(keys=[MO_code#159], functions=[partial_count(1)], schema specialized)\n",
      "   :        +- Filter (length(MO_code#159) > 0)\n",
      "   :           +- Generate explode(split(Mocodes#10,  , -1)), false, [MO_code#159]\n",
      "   :              +- Union\n",
      "   :                 :- Filter isnotnull(Mocodes#10)\n",
      "   :                 :  +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [isnotnull(Mocodes#10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   :                 +- Filter isnotnull(Mocodes#77)\n",
      "   :                    +- FileScan csv [Mocodes#77] Batched: false, DataFilters: [isnotnull(Mocodes#77)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=60]\n",
      "      +- Project [split(value#151,  , 2)[0] AS MO_code#153, split(value#151,  , 2)[1] AS MO_desc#154]\n",
      "         +- Filter ((length(split(value#151,  , 2)[0]) > 0) AND isnotnull(split(value#151,  , 2)[0]))\n",
      "            +- FileScan text [value#151] Batched: false, DataFilters: [(length(split(value#151,  , 2)[0]) > 0), isnotnull(split(value#151,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "Time taken for BROADCAST: 22.6852 seconds\n",
      "\n",
      "--- Testing Strategy: MERGE ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [MO_code#159], [MO_code#153], Inner\n",
      "   :- Sort [MO_code#159 ASC NULLS FIRST], false, 0\n",
      "   :  +- HashAggregate(keys=[MO_code#159], functions=[count(1)], schema specialized)\n",
      "   :     +- Exchange hashpartitioning(MO_code#159, 1000), ENSURE_REQUIREMENTS, [plan_id=390]\n",
      "   :        +- HashAggregate(keys=[MO_code#159], functions=[partial_count(1)], schema specialized)\n",
      "   :           +- Filter (length(MO_code#159) > 0)\n",
      "   :              +- Generate explode(split(Mocodes#10,  , -1)), false, [MO_code#159]\n",
      "   :                 +- Union\n",
      "   :                    :- Filter isnotnull(Mocodes#10)\n",
      "   :                    :  +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [isnotnull(Mocodes#10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   :                    +- Filter isnotnull(Mocodes#77)\n",
      "   :                       +- FileScan csv [Mocodes#77] Batched: false, DataFilters: [isnotnull(Mocodes#77)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   +- Sort [MO_code#153 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(MO_code#153, 1000), ENSURE_REQUIREMENTS, [plan_id=394]\n",
      "         +- Project [split(value#151,  , 2)[0] AS MO_code#153, split(value#151,  , 2)[1] AS MO_desc#154]\n",
      "            +- Filter ((length(split(value#151,  , 2)[0]) > 0) AND isnotnull(split(value#151,  , 2)[0]))\n",
      "               +- FileScan text [value#151] Batched: false, DataFilters: [(length(split(value#151,  , 2)[0]) > 0), isnotnull(split(value#151,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "Time taken for MERGE: 5.7036 seconds\n",
      "\n",
      "--- Testing Strategy: SHUFFLE_HASH ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [MO_code#159], [MO_code#153], Inner, BuildRight\n",
      "   :- HashAggregate(keys=[MO_code#159], functions=[count(1)], schema specialized)\n",
      "   :  +- Exchange hashpartitioning(MO_code#159, 1000), ENSURE_REQUIREMENTS, [plan_id=787]\n",
      "   :     +- HashAggregate(keys=[MO_code#159], functions=[partial_count(1)], schema specialized)\n",
      "   :        +- Filter (length(MO_code#159) > 0)\n",
      "   :           +- Generate explode(split(Mocodes#10,  , -1)), false, [MO_code#159]\n",
      "   :              +- Union\n",
      "   :                 :- Filter isnotnull(Mocodes#10)\n",
      "   :                 :  +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [isnotnull(Mocodes#10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   :                 +- Filter isnotnull(Mocodes#77)\n",
      "   :                    +- FileScan csv [Mocodes#77] Batched: false, DataFilters: [isnotnull(Mocodes#77)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   +- Exchange hashpartitioning(MO_code#153, 1000), ENSURE_REQUIREMENTS, [plan_id=791]\n",
      "      +- Project [split(value#151,  , 2)[0] AS MO_code#153, split(value#151,  , 2)[1] AS MO_desc#154]\n",
      "         +- Filter ((length(split(value#151,  , 2)[0]) > 0) AND isnotnull(split(value#151,  , 2)[0]))\n",
      "            +- FileScan text [value#151] Batched: false, DataFilters: [(length(split(value#151,  , 2)[0]) > 0), isnotnull(split(value#151,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "Time taken for SHUFFLE_HASH: 4.0228 seconds\n",
      "\n",
      "--- Testing Strategy: SHUFFLE_REPLICATE_NL ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- CartesianProduct (MO_code#159 = MO_code#153)\n",
      "   :- HashAggregate(keys=[MO_code#159], functions=[count(1)], schema specialized)\n",
      "   :  +- Exchange hashpartitioning(MO_code#159, 1000), ENSURE_REQUIREMENTS, [plan_id=1134]\n",
      "   :     +- HashAggregate(keys=[MO_code#159], functions=[partial_count(1)], schema specialized)\n",
      "   :        +- Filter (length(MO_code#159) > 0)\n",
      "   :           +- Generate explode(split(Mocodes#10,  , -1)), false, [MO_code#159]\n",
      "   :              +- Union\n",
      "   :                 :- Filter isnotnull(Mocodes#10)\n",
      "   :                 :  +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [isnotnull(Mocodes#10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   :                 +- Filter isnotnull(Mocodes#77)\n",
      "   :                    +- FileScan csv [Mocodes#77] Batched: false, DataFilters: [isnotnull(Mocodes#77)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   +- Project [split(value#151,  , 2)[0] AS MO_code#153, split(value#151,  , 2)[1] AS MO_desc#154]\n",
      "      +- Filter ((length(split(value#151,  , 2)[0]) > 0) AND isnotnull(split(value#151,  , 2)[0]))\n",
      "         +- FileScan text [value#151] Batched: false, DataFilters: [(length(split(value#151,  , 2)[0]) > 0), isnotnull(split(value#151,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "Time taken for SHUFFLE_REPLICATE_NL: 3.7794 seconds\n",
      "+-------+--------------------------------------------------------------------------------+-------+\n",
      "|MO_code|MO_desc                                                                         |count  |\n",
      "+-------+--------------------------------------------------------------------------------+-------+\n",
      "|0344   |Removes vict property                                                           |1002900|\n",
      "|1822   |Stranger                                                                        |548422 |\n",
      "|0416   |Hit-Hit w/ weapon                                                               |404773 |\n",
      "|0329   |Vandalized                                                                      |377536 |\n",
      "|0913   |Victim knew Suspect                                                             |278618 |\n",
      "|2000   |Domestic violence                                                               |256188 |\n",
      "|1300   |Vehicle involved                                                                |219082 |\n",
      "|0400   |Force used                                                                      |213165 |\n",
      "|1402   |Evidence Booked (any crime)                                                     |177470 |\n",
      "|1609   |Smashed                                                                         |131229 |\n",
      "|1309   |Susp uses vehicle                                                               |122108 |\n",
      "|1202   |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|120238 |\n",
      "|0325   |Took merchandise                                                                |120159 |\n",
      "|1814   |Susp is/was current/former boyfriend/girlfriend                                 |118073 |\n",
      "|0444   |Pushed                                                                          |116763 |\n",
      "|1501   |Other MO (see rpt)                                                              |115589 |\n",
      "|1307   |Breaks window                                                                   |113609 |\n",
      "|0334   |Brandishes weapon                                                               |105665 |\n",
      "|2004   |Suspect is homeless/transient                                                   |93426  |\n",
      "|0432   |Intimidation                                                                    |83562  |\n",
      "+-------+--------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Implementation 1: DataFrame API\n",
    "# Operations and Visualization\n",
    "# Here we take the Mocodes column and break it into tuple arrays\n",
    "# for each MO code we find. Then we explode the tuples to have a column\n",
    "# that has one MO code in every row.\n",
    "crime_mocodes_exploded = Crime_df.filter(col(\"Mocodes\").isNotNull()) \\\n",
    "    .select(explode(split(col(\"Mocodes\"), \" \")).alias(\"MO_code\")) \\\n",
    "    .filter(length(col(\"MO_code\")) > 0)\n",
    "# Now we count\n",
    "crime_counts = crime_mocodes_exploded.groupBy(\"MO_code\").count()\n",
    "join_strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "for strategy in join_strategies:\n",
    "    spark.catalog.clearCache()\n",
    "    print(f\"\\n--- Testing Strategy: {strategy} ---\")\n",
    "    # Apply hint() to the MO_codes_df dataframe\n",
    "    joined_df = crime_counts.join(\n",
    "        MO_codes_df.hint(strategy),\n",
    "        crime_counts[\"MO_code\"] == MO_codes_df[\"MO_code\"]\n",
    "    )\n",
    "    joined_df.explain()\n",
    "    # Time Benchmarking\n",
    "    start_time = time.time()\n",
    "    count_result = joined_df.count()\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for {strategy}: {end_time - start_time:.4f} seconds\")\n",
    "final_result_df = crime_counts \\\n",
    "    .join(MO_codes_df.hint(\"BROADCAST\"), \"MO_code\") \\\n",
    "    .select(\"MO_code\", \"MO_desc\", \"count\") \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "final_result_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f7de35-4dba-41e4-a751-a24204154093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a2e38745e64f53a681d8663c31e6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Execution Time: 38.0870 seconds\n",
      "\n",
      "--- RDD Results ---\n",
      "Code: 0344, Count: 1002900, Desc: Removes vict property\n",
      "Code: 1822, Count: 548422, Desc: Stranger\n",
      "Code: 0416, Count: 404773, Desc: Hit-Hit w/ weapon\n",
      "Code: 0329, Count: 377536, Desc: Vandalized\n",
      "Code: 0913, Count: 278618, Desc: Victim knew Suspect\n",
      "Code: 2000, Count: 256188, Desc: Domestic violence\n",
      "Code: 1300, Count: 219082, Desc: Vehicle involved\n",
      "Code: 0400, Count: 213165, Desc: Force used\n",
      "Code: 1402, Count: 177470, Desc: Evidence Booked (any crime)\n",
      "Code: 1609, Count: 131229, Desc: Smashed\n",
      "Code: 1309, Count: 122108, Desc: Susp uses vehicle\n",
      "Code: 1202, Count: 120238, Desc: Victim was aged (60 & over) or blind/physically disabled/unable to care for self\n",
      "Code: 0325, Count: 120159, Desc: Took merchandise\n",
      "Code: 1814, Count: 118073, Desc: Susp is/was current/former boyfriend/girlfriend\n",
      "Code: 0444, Count: 116763, Desc: Pushed\n",
      "Code: 1501, Count: 115589, Desc: Other MO (see rpt)\n",
      "Code: 1307, Count: 113609, Desc: Breaks window\n",
      "Code: 0334, Count: 105665, Desc: Brandishes weapon\n",
      "Code: 2004, Count: 93426, Desc: Suspect is homeless/transient\n",
      "Code: 0432, Count: 83562, Desc: Intimidation"
     ]
    }
   ],
   "source": [
    "# Implementation 2: RDD API\n",
    "spark.catalog.clearCache()\n",
    "# We take only the Mocodes column from Crime_df\n",
    "crime_rdd = Crime_df.select(\"Mocodes\").rdd.filter(lambda x: x[0] is not None)\n",
    "# We take both the code and the description from the MO_codes_df\n",
    "mo_rdd = MO_codes_df.rdd.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Map & Reduce\n",
    "# flatMap: We break the string \"0416 0334\" in a list [0416, 0334]\n",
    "# and then many records\n",
    "counts_rdd = crime_rdd \\\n",
    "    .flatMap(lambda row: row[0].split(\" \")) \\\n",
    "    .filter(lambda code: len(code) > 0) \\\n",
    "    .map(lambda code: (code, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Join with RDDs: Requires (Key, Value), where the Key\n",
    "# in both our RDDs is the MO code\n",
    "joined_rdd = counts_rdd.join(mo_rdd)\n",
    "# Sorting\n",
    "# The result of the join is (Code, (Count, Description))\n",
    "# and we want sorting based on Count (1st element of value tuple)\n",
    "final_rdd = joined_rdd.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "# Time Benchmarking\n",
    "start_time = time.time()\n",
    "total_rows = final_rdd.count()\n",
    "end_time = time.time()\n",
    "print(f\"RDD Execution Time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# Visualization\n",
    "print(\"\\n--- RDD Results ---\")\n",
    "for row in final_rdd.take(20):\n",
    "    print(f\"Code: {row[0]}, Count: {row[1][0]}, Desc: {row[1][1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
