{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f662188-fa3b-4ae2-8cf7-75d071d74ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>254</td><td>application_1765289937462_0251</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0251/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0251_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>260</td><td>application_1765289937462_0257</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0257/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0257_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>274</td><td>application_1765289937462_0271</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0271/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0271_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>284</td><td>application_1765289937462_0281</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0281/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0281_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>294</td><td>application_1765289937462_0291</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0291/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0291_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>299</td><td>application_1765289937462_0296</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0296/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0296_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbf81fb-78ee-4b3e-97d8-58dc2261618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>301</td><td>application_1765289937462_0298</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0298/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0298_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8650ef46e9c64bde9d2aaaa50aafd6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107c7596d65849748e2f281c55b13dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, to_date, count, sum as _sum, corr, desc, lit\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, DoubleType, FloatType\n",
    "\n",
    "# Sedona Imports\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 4 execution\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.sedona_sql.io.SedonaSqlWrapper\") \\\n",
    "    .getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3360d1de-8ff5-4f26-8d7d-5746b52e3555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc819f7b2424d68b0a9012fdfb2983b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing the data\n",
    "Crime_data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "Recent_crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "                                      header = True, \\\n",
    "                                      schema = Crime_data_schema)\n",
    "Older_crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "                                     header = True, \\\n",
    "                                     schema = Crime_data_schema)\n",
    "Crime_df = Recent_crime_data_df.union(Older_crime_data_df)\n",
    "\n",
    "# Load stations\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\",\n",
    "                                 header=True,\n",
    "                                 inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b76968-4a3b-422f-bd65-8913224443c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76b49fa26004f3fb03f0b206744f26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (13)\n",
      "+- Project (12)\n",
      "   +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "      :- Union (7)\n",
      "      :  :- Project (3)\n",
      "      :  :  +- Filter (2)\n",
      "      :  :     +- Scan csv  (1)\n",
      "      :  +- Project (6)\n",
      "      :     +- Filter (5)\n",
      "      :        +- Scan csv  (4)\n",
      "      +- BroadcastExchange (10)\n",
      "         +- Project (9)\n",
      "            +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:float,LON:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "Condition : (((isnotnull(LAT#50) AND isnotnull(LON#51)) AND NOT (LAT#50 = 0.0)) AND NOT (LON#51 = 0.0))\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#194]\n",
      "Input [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:float,LON:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "Condition : (((isnotnull(LAT#106) AND isnotnull(LON#107)) AND NOT (LAT#106 = 0.0)) AND NOT (LON#107 = 0.0))\n",
      "\n",
      "(6) Project\n",
      "Output [29]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#304]\n",
      "Input [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#182, Y#183, DIVISION#185]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [4]: [DIVISION#185 AS division#224, X#182 AS station_lon#225, Y#183 AS station_lat#226,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#231]\n",
      "Input [3]: [X#182, Y#183, DIVISION#185]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [4]: [division#224, station_lon#225, station_lat#226, station_geom#231]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=86]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [34]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#269]\n",
      "Input [33]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231]\n",
      "\n",
      "(13) AdaptiveSparkPlan\n",
      "Output [34]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231, distance#269]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+----------------+----------------+------+\n",
      "|division        |average_distance|#     |\n",
      "+----------------+----------------+------+\n",
      "|HOLLYWOOD       |0.02            |212878|\n",
      "|VAN NUYS        |0.029           |209305|\n",
      "|WILSHIRE        |0.026           |198499|\n",
      "|SOUTHWEST       |0.022           |186976|\n",
      "|OLYMPIC         |0.017           |172251|\n",
      "|NORTH HOLLYWOOD |0.026           |171399|\n",
      "|77TH STREET     |0.017           |166133|\n",
      "|PACIFIC         |0.038           |158098|\n",
      "|CENTRAL         |0.01            |155274|\n",
      "|RAMPART         |0.015           |150304|\n",
      "|SOUTHEAST       |0.024           |143597|\n",
      "|TOPANGA         |0.032           |139462|\n",
      "|WEST VALLEY     |0.029           |129457|\n",
      "|HARBOR          |0.035           |127073|\n",
      "|WEST LOS ANGELES|0.03            |121301|\n",
      "|FOOTHILL        |0.041           |120663|\n",
      "|HOLLENBECK      |0.026           |119726|\n",
      "|NEWTON          |0.016           |109339|\n",
      "|NORTHEAST       |0.039           |105837|\n",
      "|MISSION         |0.035           |103198|\n",
      "|DEVONSHIRE      |0.028           |76403 |\n",
      "+----------------+----------------+------+\n",
      "\n",
      "\n",
      "Execution time: 50.46 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (26)\n",
      "+- Sort (25)\n",
      "   +- Exchange (24)\n",
      "      +- HashAggregate (23)\n",
      "         +- Exchange (22)\n",
      "            +- HashAggregate (21)\n",
      "               +- Project (20)\n",
      "                  +- Filter (19)\n",
      "                     +- Window (18)\n",
      "                        +- WindowGroupLimit (17)\n",
      "                           +- Sort (16)\n",
      "                              +- Exchange (15)\n",
      "                                 +- WindowGroupLimit (14)\n",
      "                                    +- Sort (13)\n",
      "                                       +- Project (12)\n",
      "                                          +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "                                             :- Union (7)\n",
      "                                             :  :- Project (3)\n",
      "                                             :  :  +- Filter (2)\n",
      "                                             :  :     +- Scan csv  (1)\n",
      "                                             :  +- Project (6)\n",
      "                                             :     +- Filter (5)\n",
      "                                             :        +- Scan csv  (4)\n",
      "                                             +- BroadcastExchange (10)\n",
      "                                                +- Project (9)\n",
      "                                                   +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,LAT:float,LON:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "Condition : (((isnotnull(LAT#50) AND isnotnull(LON#51)) AND NOT (LAT#50 = 0.0)) AND NOT (LON#51 = 0.0))\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [DR_NO#24,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#194]\n",
      "Input [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,LAT:float,LON:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "Condition : (((isnotnull(LAT#106) AND isnotnull(LON#107)) AND NOT (LAT#106 = 0.0)) AND NOT (LON#107 = 0.0))\n",
      "\n",
      "(6) Project\n",
      "Output [2]: [DR_NO#80,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#484]\n",
      "Input [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#182, Y#183, DIVISION#185]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [DIVISION#185 AS division#224,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#231]\n",
      "Input [3]: [X#182, Y#183, DIVISION#185]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [division#224, station_geom#231]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=621]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [DR_NO#24, division#224,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#269]\n",
      "Input [4]: [DR_NO#24, crime_geom#194, division#224, station_geom#231]\n",
      "\n",
      "(13) Sort\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24 ASC NULLS FIRST, distance#269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(14) WindowGroupLimit\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24], [distance#269 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "\n",
      "(15) Exchange\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: hashpartitioning(DR_NO#24, 1000), ENSURE_REQUIREMENTS, [plan_id=628]\n",
      "\n",
      "(16) Sort\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24 ASC NULLS FIRST, distance#269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(17) WindowGroupLimit\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24], [distance#269 ASC NULLS FIRST], row_number(), 1, Final\n",
      "\n",
      "(18) Window\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [row_number() windowspecdefinition(DR_NO#24, distance#269 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#306], [DR_NO#24], [distance#269 ASC NULLS FIRST]\n",
      "\n",
      "(19) Filter\n",
      "Input [4]: [DR_NO#24, division#224, distance#269, rn#306]\n",
      "Condition : (rn#306 = 1)\n",
      "\n",
      "(20) Project\n",
      "Output [2]: [division#224, distance#269]\n",
      "Input [4]: [DR_NO#24, division#224, distance#269, rn#306]\n",
      "\n",
      "(21) HashAggregate\n",
      "Input [2]: [division#224, distance#269]\n",
      "Keys [1]: [division#224]\n",
      "Functions [2]: [partial_avg(distance#269), partial_count(1)]\n",
      "Aggregate Attributes [3]: [sum#397, count#398L, count#401L]\n",
      "Results [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "\n",
      "(22) Exchange\n",
      "Input [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "Arguments: hashpartitioning(division#224, 1000), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "\n",
      "(23) HashAggregate\n",
      "Input [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "Keys [1]: [division#224]\n",
      "Functions [2]: [avg(distance#269), count(1)]\n",
      "Aggregate Attributes [2]: [avg(distance#269)#377, count(1)#379L]\n",
      "Results [3]: [division#224, round(avg(distance#269)#377, 3) AS average_distance#378, count(1)#379L AS ##380L]\n",
      "\n",
      "(24) Exchange\n",
      "Input [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: rangepartitioning(##380L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=639]\n",
      "\n",
      "(25) Sort\n",
      "Input [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: [##380L DESC NULLS LAST], true, 0\n",
      "\n",
      "(26) AdaptiveSparkPlan\n",
      "Output [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# Implementation using 1 core, 2 GB RAM\n",
    "\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "spark.catalog.clearCache()\n",
    "# Clean crime data (remove Null Island)\n",
    "\n",
    "crimes_filtered = Crime_df.filter(\n",
    "    (F.col(\"LAT\").isNotNull()) &\n",
    "    (F.col(\"LON\").isNotNull()) &\n",
    "    (F.col(\"LAT\") != 0.0) &\n",
    "    (F.col(\"LON\") != 0.0)\n",
    ")\n",
    "\n",
    "# Create geometry column\n",
    "crime_geom = crimes_filtered.withColumn(\n",
    "    \"crime_geom\",\n",
    "    F.expr(\"ST_Point(CAST(LON AS DOUBLE), CAST(LAT AS DOUBLE))\")\n",
    ")\n",
    "\n",
    "# Prepare police stations\n",
    "\n",
    "stations = stations_df.select(\n",
    "    F.col(\"DIVISION\").alias(\"division\"),\n",
    "    F.col(\"X\").cast(\"double\").alias(\"station_lon\"),\n",
    "    F.col(\"Y\").cast(\"double\").alias(\"station_lat\")\n",
    ")\n",
    "\n",
    "stations_geom = stations.withColumn(\n",
    "    \"station_geom\",\n",
    "    F.expr(\"ST_Point(station_lon, station_lat)\")\n",
    ")\n",
    "\n",
    "# Cross join + compute distances\n",
    "\n",
    "joined = crime_geom.crossJoin(F.broadcast(stations_geom)) \\\n",
    "    .withColumn(\n",
    "        \"distance\",\n",
    "        F.expr(\"ST_Distance(crime_geom, station_geom)\")\n",
    "    )\n",
    "\n",
    "joined.explain(\"formatted\")\n",
    "\n",
    "# Window → nearest police station per crime\n",
    "\n",
    "w = Window.partitionBy(\"DR_NO\").orderBy(F.col(\"distance\").asc())\n",
    "\n",
    "nearest_station = joined.withColumn(\n",
    "    \"rn\",\n",
    "    F.row_number().over(w)\n",
    ").filter(F.col(\"rn\") == 1)\n",
    "\n",
    "# Aggregate results per police division\n",
    "\n",
    "result = nearest_station.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.round(F.avg(\"distance\"), 3).alias(\"average_distance\"),\n",
    "        F.count(\"*\").alias(\"#\")\n",
    "    ) \\\n",
    "    .select(\"division\", \"average_distance\", \"#\") \\\n",
    "    .orderBy(F.col(\"#\").desc())\n",
    "\n",
    "# Execute + Measure time\n",
    "\n",
    "start = time.time()\n",
    "result.show(21, truncate=False)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nExecution time: {end - start:.2f} seconds\")\n",
    "\n",
    "# Explain final execution plan\n",
    "\n",
    "result.explain(\"formatted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7361c433-b7ff-4123-a534-6522037ee057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1505eb0a994b1b8b467f6dc7e21428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (13)\n",
      "+- Project (12)\n",
      "   +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "      :- Union (7)\n",
      "      :  :- Project (3)\n",
      "      :  :  +- Filter (2)\n",
      "      :  :     +- Scan csv  (1)\n",
      "      :  +- Project (6)\n",
      "      :     +- Filter (5)\n",
      "      :        +- Scan csv  (4)\n",
      "      +- BroadcastExchange (10)\n",
      "         +- Project (9)\n",
      "            +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:float,LON:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "Condition : (((isnotnull(LAT#50) AND isnotnull(LON#51)) AND NOT (LAT#50 = 0.0)) AND NOT (LON#51 = 0.0))\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#194]\n",
      "Input [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:float,LON:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "Condition : (((isnotnull(LAT#106) AND isnotnull(LON#107)) AND NOT (LAT#106 = 0.0)) AND NOT (LON#107 = 0.0))\n",
      "\n",
      "(6) Project\n",
      "Output [29]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#304]\n",
      "Input [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#182, Y#183, DIVISION#185]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [4]: [DIVISION#185 AS division#224, X#182 AS station_lon#225, Y#183 AS station_lat#226,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#231]\n",
      "Input [3]: [X#182, Y#183, DIVISION#185]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [4]: [division#224, station_lon#225, station_lat#226, station_geom#231]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=86]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [34]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#269]\n",
      "Input [33]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231]\n",
      "\n",
      "(13) AdaptiveSparkPlan\n",
      "Output [34]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231, distance#269]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+----------------+----------------+------+\n",
      "|division        |average_distance|#     |\n",
      "+----------------+----------------+------+\n",
      "|HOLLYWOOD       |0.02            |212878|\n",
      "|VAN NUYS        |0.029           |209305|\n",
      "|WILSHIRE        |0.026           |198499|\n",
      "|SOUTHWEST       |0.022           |186976|\n",
      "|OLYMPIC         |0.017           |172251|\n",
      "|NORTH HOLLYWOOD |0.026           |171399|\n",
      "|77TH STREET     |0.017           |166133|\n",
      "|PACIFIC         |0.038           |158098|\n",
      "|CENTRAL         |0.01            |155274|\n",
      "|RAMPART         |0.015           |150304|\n",
      "|SOUTHEAST       |0.024           |143597|\n",
      "|TOPANGA         |0.032           |139462|\n",
      "|WEST VALLEY     |0.029           |129457|\n",
      "|HARBOR          |0.035           |127073|\n",
      "|WEST LOS ANGELES|0.03            |121301|\n",
      "|FOOTHILL        |0.041           |120663|\n",
      "|HOLLENBECK      |0.026           |119726|\n",
      "|NEWTON          |0.016           |109339|\n",
      "|NORTHEAST       |0.039           |105837|\n",
      "|MISSION         |0.035           |103198|\n",
      "|DEVONSHIRE      |0.028           |76403 |\n",
      "+----------------+----------------+------+\n",
      "\n",
      "\n",
      "Execution time: 39.55 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (26)\n",
      "+- Sort (25)\n",
      "   +- Exchange (24)\n",
      "      +- HashAggregate (23)\n",
      "         +- Exchange (22)\n",
      "            +- HashAggregate (21)\n",
      "               +- Project (20)\n",
      "                  +- Filter (19)\n",
      "                     +- Window (18)\n",
      "                        +- WindowGroupLimit (17)\n",
      "                           +- Sort (16)\n",
      "                              +- Exchange (15)\n",
      "                                 +- WindowGroupLimit (14)\n",
      "                                    +- Sort (13)\n",
      "                                       +- Project (12)\n",
      "                                          +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "                                             :- Union (7)\n",
      "                                             :  :- Project (3)\n",
      "                                             :  :  +- Filter (2)\n",
      "                                             :  :     +- Scan csv  (1)\n",
      "                                             :  +- Project (6)\n",
      "                                             :     +- Filter (5)\n",
      "                                             :        +- Scan csv  (4)\n",
      "                                             +- BroadcastExchange (10)\n",
      "                                                +- Project (9)\n",
      "                                                   +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,LAT:float,LON:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "Condition : (((isnotnull(LAT#50) AND isnotnull(LON#51)) AND NOT (LAT#50 = 0.0)) AND NOT (LON#51 = 0.0))\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [DR_NO#24,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#194]\n",
      "Input [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,LAT:float,LON:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "Condition : (((isnotnull(LAT#106) AND isnotnull(LON#107)) AND NOT (LAT#106 = 0.0)) AND NOT (LON#107 = 0.0))\n",
      "\n",
      "(6) Project\n",
      "Output [2]: [DR_NO#80,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#484]\n",
      "Input [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#182, Y#183, DIVISION#185]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [DIVISION#185 AS division#224,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#231]\n",
      "Input [3]: [X#182, Y#183, DIVISION#185]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [division#224, station_geom#231]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=621]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [DR_NO#24, division#224,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#269]\n",
      "Input [4]: [DR_NO#24, crime_geom#194, division#224, station_geom#231]\n",
      "\n",
      "(13) Sort\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24 ASC NULLS FIRST, distance#269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(14) WindowGroupLimit\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24], [distance#269 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "\n",
      "(15) Exchange\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: hashpartitioning(DR_NO#24, 1000), ENSURE_REQUIREMENTS, [plan_id=628]\n",
      "\n",
      "(16) Sort\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24 ASC NULLS FIRST, distance#269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(17) WindowGroupLimit\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24], [distance#269 ASC NULLS FIRST], row_number(), 1, Final\n",
      "\n",
      "(18) Window\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [row_number() windowspecdefinition(DR_NO#24, distance#269 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#306], [DR_NO#24], [distance#269 ASC NULLS FIRST]\n",
      "\n",
      "(19) Filter\n",
      "Input [4]: [DR_NO#24, division#224, distance#269, rn#306]\n",
      "Condition : (rn#306 = 1)\n",
      "\n",
      "(20) Project\n",
      "Output [2]: [division#224, distance#269]\n",
      "Input [4]: [DR_NO#24, division#224, distance#269, rn#306]\n",
      "\n",
      "(21) HashAggregate\n",
      "Input [2]: [division#224, distance#269]\n",
      "Keys [1]: [division#224]\n",
      "Functions [2]: [partial_avg(distance#269), partial_count(1)]\n",
      "Aggregate Attributes [3]: [sum#397, count#398L, count#401L]\n",
      "Results [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "\n",
      "(22) Exchange\n",
      "Input [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "Arguments: hashpartitioning(division#224, 1000), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "\n",
      "(23) HashAggregate\n",
      "Input [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "Keys [1]: [division#224]\n",
      "Functions [2]: [avg(distance#269), count(1)]\n",
      "Aggregate Attributes [2]: [avg(distance#269)#377, count(1)#379L]\n",
      "Results [3]: [division#224, round(avg(distance#269)#377, 3) AS average_distance#378, count(1)#379L AS ##380L]\n",
      "\n",
      "(24) Exchange\n",
      "Input [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: rangepartitioning(##380L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=639]\n",
      "\n",
      "(25) Sort\n",
      "Input [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: [##380L DESC NULLS LAST], true, 0\n",
      "\n",
      "(26) AdaptiveSparkPlan\n",
      "Output [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# Implementation using 2 cores, 4 GB RAM\n",
    "\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "spark.catalog.clearCache()\n",
    "# Clean crime data (remove Null Island)\n",
    "\n",
    "crimes_filtered = Crime_df.filter(\n",
    "    (F.col(\"LAT\").isNotNull()) &\n",
    "    (F.col(\"LON\").isNotNull()) &\n",
    "    (F.col(\"LAT\") != 0.0) &\n",
    "    (F.col(\"LON\") != 0.0)\n",
    ")\n",
    "\n",
    "# Create geometry column\n",
    "crime_geom = crimes_filtered.withColumn(\n",
    "    \"crime_geom\",\n",
    "    F.expr(\"ST_Point(CAST(LON AS DOUBLE), CAST(LAT AS DOUBLE))\")\n",
    ")\n",
    "\n",
    "# Prepare police stations\n",
    "\n",
    "stations = stations_df.select(\n",
    "    F.col(\"DIVISION\").alias(\"division\"),\n",
    "    F.col(\"X\").cast(\"double\").alias(\"station_lon\"),\n",
    "    F.col(\"Y\").cast(\"double\").alias(\"station_lat\")\n",
    ")\n",
    "\n",
    "stations_geom = stations.withColumn(\n",
    "    \"station_geom\",\n",
    "    F.expr(\"ST_Point(station_lon, station_lat)\")\n",
    ")\n",
    "\n",
    "# Cross join + compute distances\n",
    "\n",
    "joined = crime_geom.crossJoin(F.broadcast(stations_geom)) \\\n",
    "    .withColumn(\n",
    "        \"distance\",\n",
    "        F.expr(\"ST_Distance(crime_geom, station_geom)\")\n",
    "    )\n",
    "\n",
    "joined.explain(\"formatted\")\n",
    "\n",
    "# Window → nearest police station per crime\n",
    "\n",
    "w = Window.partitionBy(\"DR_NO\").orderBy(F.col(\"distance\").asc())\n",
    "\n",
    "nearest_station = joined.withColumn(\n",
    "    \"rn\",\n",
    "    F.row_number().over(w)\n",
    ").filter(F.col(\"rn\") == 1)\n",
    "\n",
    "# Aggregate results per police division\n",
    "\n",
    "result = nearest_station.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.round(F.avg(\"distance\"), 3).alias(\"average_distance\"),\n",
    "        F.count(\"*\").alias(\"#\")\n",
    "    ) \\\n",
    "    .select(\"division\", \"average_distance\", \"#\") \\\n",
    "    .orderBy(F.col(\"#\").desc())\n",
    "\n",
    "# Execute + Measure time\n",
    "\n",
    "start = time.time()\n",
    "result.show(21, truncate=False)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nExecution time: {end - start:.2f} seconds\")\n",
    "\n",
    "# Explain final execution plan\n",
    "\n",
    "result.explain(\"formatted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbe5352-c14d-4bee-b36b-16f1532c922f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a216fe732fe94686b704abb45ec87f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (13)\n",
      "+- Project (12)\n",
      "   +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "      :- Union (7)\n",
      "      :  :- Project (3)\n",
      "      :  :  +- Filter (2)\n",
      "      :  :     +- Scan csv  (1)\n",
      "      :  +- Project (6)\n",
      "      :     +- Filter (5)\n",
      "      :        +- Scan csv  (4)\n",
      "      +- BroadcastExchange (10)\n",
      "         +- Project (9)\n",
      "            +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:float,LON:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "Condition : (((isnotnull(LAT#50) AND isnotnull(LON#51)) AND NOT (LAT#50 = 0.0)) AND NOT (LON#51 = 0.0))\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#194]\n",
      "Input [28]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:float,LON:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "Condition : (((isnotnull(LAT#106) AND isnotnull(LON#107)) AND NOT (LAT#106 = 0.0)) AND NOT (LON#107 = 0.0))\n",
      "\n",
      "(6) Project\n",
      "Output [29]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#304]\n",
      "Input [28]: [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, LOCATION#104, Cross Street#105, LAT#106, LON#107]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#182, Y#183, DIVISION#185]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [4]: [DIVISION#185 AS division#224, X#182 AS station_lon#225, Y#183 AS station_lat#226,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#231]\n",
      "Input [3]: [X#182, Y#183, DIVISION#185]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [4]: [division#224, station_lon#225, station_lat#226, station_geom#231]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=86]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [34]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#269]\n",
      "Input [33]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231]\n",
      "\n",
      "(13) AdaptiveSparkPlan\n",
      "Output [34]: [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, LOCATION#48, Cross Street#49, LAT#50, LON#51, crime_geom#194, division#224, station_lon#225, station_lat#226, station_geom#231, distance#269]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+----------------+----------------+------+\n",
      "|division        |average_distance|#     |\n",
      "+----------------+----------------+------+\n",
      "|HOLLYWOOD       |0.02            |212878|\n",
      "|VAN NUYS        |0.029           |209305|\n",
      "|WILSHIRE        |0.026           |198499|\n",
      "|SOUTHWEST       |0.022           |186976|\n",
      "|OLYMPIC         |0.017           |172251|\n",
      "|NORTH HOLLYWOOD |0.026           |171399|\n",
      "|77TH STREET     |0.017           |166133|\n",
      "|PACIFIC         |0.038           |158098|\n",
      "|CENTRAL         |0.01            |155274|\n",
      "|RAMPART         |0.015           |150304|\n",
      "|SOUTHEAST       |0.024           |143597|\n",
      "|TOPANGA         |0.032           |139462|\n",
      "|WEST VALLEY     |0.029           |129457|\n",
      "|HARBOR          |0.035           |127073|\n",
      "|WEST LOS ANGELES|0.03            |121301|\n",
      "|FOOTHILL        |0.041           |120663|\n",
      "|HOLLENBECK      |0.026           |119726|\n",
      "|NEWTON          |0.016           |109339|\n",
      "|NORTHEAST       |0.039           |105837|\n",
      "|MISSION         |0.035           |103198|\n",
      "|DEVONSHIRE      |0.028           |76403 |\n",
      "+----------------+----------------+------+\n",
      "\n",
      "\n",
      "Execution time: 29.57 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (26)\n",
      "+- Sort (25)\n",
      "   +- Exchange (24)\n",
      "      +- HashAggregate (23)\n",
      "         +- Exchange (22)\n",
      "            +- HashAggregate (21)\n",
      "               +- Project (20)\n",
      "                  +- Filter (19)\n",
      "                     +- Window (18)\n",
      "                        +- WindowGroupLimit (17)\n",
      "                           +- Sort (16)\n",
      "                              +- Exchange (15)\n",
      "                                 +- WindowGroupLimit (14)\n",
      "                                    +- Sort (13)\n",
      "                                       +- Project (12)\n",
      "                                          +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "                                             :- Union (7)\n",
      "                                             :  :- Project (3)\n",
      "                                             :  :  +- Filter (2)\n",
      "                                             :  :     +- Scan csv  (1)\n",
      "                                             :  +- Project (6)\n",
      "                                             :     +- Filter (5)\n",
      "                                             :        +- Scan csv  (4)\n",
      "                                             +- BroadcastExchange (10)\n",
      "                                                +- Project (9)\n",
      "                                                   +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,LAT:float,LON:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "Condition : (((isnotnull(LAT#50) AND isnotnull(LON#51)) AND NOT (LAT#50 = 0.0)) AND NOT (LON#51 = 0.0))\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [DR_NO#24,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#194]\n",
      "Input [3]: [DR_NO#24, LAT#50, LON#51]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))]\n",
      "ReadSchema: struct<DR_NO:int,LAT:float,LON:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "Condition : (((isnotnull(LAT#106) AND isnotnull(LON#107)) AND NOT (LAT#106 = 0.0)) AND NOT (LON#107 = 0.0))\n",
      "\n",
      "(6) Project\n",
      "Output [2]: [DR_NO#80,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#484]\n",
      "Input [3]: [DR_NO#80, LAT#106, LON#107]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#182, Y#183, DIVISION#185]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [DIVISION#185 AS division#224,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#231]\n",
      "Input [3]: [X#182, Y#183, DIVISION#185]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [division#224, station_geom#231]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=621]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [DR_NO#24, division#224,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#269]\n",
      "Input [4]: [DR_NO#24, crime_geom#194, division#224, station_geom#231]\n",
      "\n",
      "(13) Sort\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24 ASC NULLS FIRST, distance#269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(14) WindowGroupLimit\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24], [distance#269 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "\n",
      "(15) Exchange\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: hashpartitioning(DR_NO#24, 1000), ENSURE_REQUIREMENTS, [plan_id=628]\n",
      "\n",
      "(16) Sort\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24 ASC NULLS FIRST, distance#269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(17) WindowGroupLimit\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [DR_NO#24], [distance#269 ASC NULLS FIRST], row_number(), 1, Final\n",
      "\n",
      "(18) Window\n",
      "Input [3]: [DR_NO#24, division#224, distance#269]\n",
      "Arguments: [row_number() windowspecdefinition(DR_NO#24, distance#269 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#306], [DR_NO#24], [distance#269 ASC NULLS FIRST]\n",
      "\n",
      "(19) Filter\n",
      "Input [4]: [DR_NO#24, division#224, distance#269, rn#306]\n",
      "Condition : (rn#306 = 1)\n",
      "\n",
      "(20) Project\n",
      "Output [2]: [division#224, distance#269]\n",
      "Input [4]: [DR_NO#24, division#224, distance#269, rn#306]\n",
      "\n",
      "(21) HashAggregate\n",
      "Input [2]: [division#224, distance#269]\n",
      "Keys [1]: [division#224]\n",
      "Functions [2]: [partial_avg(distance#269), partial_count(1)]\n",
      "Aggregate Attributes [3]: [sum#397, count#398L, count#401L]\n",
      "Results [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "\n",
      "(22) Exchange\n",
      "Input [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "Arguments: hashpartitioning(division#224, 1000), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "\n",
      "(23) HashAggregate\n",
      "Input [4]: [division#224, sum#399, count#400L, count#402L]\n",
      "Keys [1]: [division#224]\n",
      "Functions [2]: [avg(distance#269), count(1)]\n",
      "Aggregate Attributes [2]: [avg(distance#269)#377, count(1)#379L]\n",
      "Results [3]: [division#224, round(avg(distance#269)#377, 3) AS average_distance#378, count(1)#379L AS ##380L]\n",
      "\n",
      "(24) Exchange\n",
      "Input [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: rangepartitioning(##380L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=639]\n",
      "\n",
      "(25) Sort\n",
      "Input [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: [##380L DESC NULLS LAST], true, 0\n",
      "\n",
      "(26) AdaptiveSparkPlan\n",
      "Output [3]: [division#224, average_distance#378, ##380L]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# Implementation using 4 cores, 8 GB RAM\n",
    "\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "spark.catalog.clearCache()\n",
    "# Clean crime data (remove Null Island)\n",
    "\n",
    "crimes_filtered = Crime_df.filter(\n",
    "    (F.col(\"LAT\").isNotNull()) &\n",
    "    (F.col(\"LON\").isNotNull()) &\n",
    "    (F.col(\"LAT\") != 0.0) &\n",
    "    (F.col(\"LON\") != 0.0)\n",
    ")\n",
    "\n",
    "# Create geometry column\n",
    "crime_geom = crimes_filtered.withColumn(\n",
    "    \"crime_geom\",\n",
    "    F.expr(\"ST_Point(CAST(LON AS DOUBLE), CAST(LAT AS DOUBLE))\")\n",
    ")\n",
    "\n",
    "# Prepare police stations\n",
    "\n",
    "stations = stations_df.select(\n",
    "    F.col(\"DIVISION\").alias(\"division\"),\n",
    "    F.col(\"X\").cast(\"double\").alias(\"station_lon\"),\n",
    "    F.col(\"Y\").cast(\"double\").alias(\"station_lat\")\n",
    ")\n",
    "\n",
    "stations_geom = stations.withColumn(\n",
    "    \"station_geom\",\n",
    "    F.expr(\"ST_Point(station_lon, station_lat)\")\n",
    ")\n",
    "\n",
    "# Cross join + compute distances\n",
    "\n",
    "joined = crime_geom.crossJoin(F.broadcast(stations_geom)) \\\n",
    "    .withColumn(\n",
    "        \"distance\",\n",
    "        F.expr(\"ST_Distance(crime_geom, station_geom)\")\n",
    "    )\n",
    "\n",
    "joined.explain(\"formatted\")\n",
    "\n",
    "# Window → nearest police station per crime\n",
    "\n",
    "w = Window.partitionBy(\"DR_NO\").orderBy(F.col(\"distance\").asc())\n",
    "\n",
    "nearest_station = joined.withColumn(\n",
    "    \"rn\",\n",
    "    F.row_number().over(w)\n",
    ").filter(F.col(\"rn\") == 1)\n",
    "\n",
    "# Aggregate results per police division\n",
    "\n",
    "result = nearest_station.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.round(F.avg(\"distance\"), 3).alias(\"average_distance\"),\n",
    "        F.count(\"*\").alias(\"#\")\n",
    "    ) \\\n",
    "    .select(\"division\", \"average_distance\", \"#\") \\\n",
    "    .orderBy(F.col(\"#\").desc())\n",
    "\n",
    "# Execute + Measure time\n",
    "\n",
    "start = time.time()\n",
    "result.show(21, truncate=False)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nExecution time: {end - start:.2f} seconds\")\n",
    "\n",
    "# Explain final execution plan\n",
    "\n",
    "result.explain(\"formatted\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
