{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a6022d-1e00-4602-9815-ef83910a603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>501</td><td>application_1765289937462_0495</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0495/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0495_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>507</td><td>application_1765289937462_0501</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0501/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0501_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e454dd4e-5603-4615-a0b2-13ff36f5444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>509</td><td>application_1765289937462_0503</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0503/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0503_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe86c37c9f54498ae0e991d6a93babc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed61f1324bbe4b21b50d4713b8020fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, to_date, year, count, desc, rank, sum as _sum, round\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 execution\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b8b624-1435-4de2-ae75-d20094698143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f51a2251374ea296dec6874e2dc058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing the data\n",
    "Crime_data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "Recent_crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "                                      header = True, \\\n",
    "                                      schema = Crime_data_schema)\n",
    "Older_crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "                                     header = True, \\\n",
    "                                     schema = Crime_data_schema)\n",
    "Crime_df = Recent_crime_data_df.union(Older_crime_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546af3ee-e2bf-46dc-acc1-98b74b717305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fb2e09251e4604bbb9bc2a1f0bf7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Time: 11.0353 seconds\n",
      "\n",
      "+------------+------+\n",
      "|   age_group| count|\n",
      "+------------+------+\n",
      "|      Adults|121660|\n",
      "|Young Adults| 33758|\n",
      "|    Children| 16014|\n",
      "|     Elderly|  6011|\n",
      "+------------+------+"
     ]
    }
   ],
   "source": [
    "# Implementation 1: Dataframe API (no UDF)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Filter only incidents that contain \"aggravated assault\"\n",
    "assault = Crime_df.filter(\n",
    "    F.lower(F.col(\"Crm Cd Desc\")).contains(\"aggravated assault\")\n",
    ")\n",
    "\n",
    "# Ensure the age column is integer \n",
    "assault = assault.withColumn(\"Vict Age\", F.col(\"Vict Age\").cast(\"int\"))\n",
    "\n",
    "# Create the age groups\n",
    "assault_with_group = assault.withColumn(\n",
    "    \"age_group\",\n",
    "    F.when(F.col(\"Vict Age\") < 18, \"Children\")\n",
    "     .when((F.col(\"Vict Age\") >= 18) & (F.col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "     .when((F.col(\"Vict Age\") >= 25) & (F.col(\"Vict Age\") <= 64), \"Adults\")\n",
    "     .when(F.col(\"Vict Age\") > 64, \"Elderly\")\n",
    ")\n",
    "\n",
    "# Count incidents per age group and sort in descending order\n",
    "query_1 = (\n",
    "    assault_with_group\n",
    "    .groupBy(\"age_group\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "result = query_1.collect()   \n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nExecution Time: {end_time - start_time:.4f} seconds\\n\")\n",
    "\n",
    "# Display the results\n",
    "query_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d83ba8f-5487-48ed-90c9-46bc49de0ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a486bc787e945789052f04f37d2397b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Time: 15.1313 seconds\n",
      "\n",
      "+------------+------+\n",
      "|   age_group| count|\n",
      "+------------+------+\n",
      "|      Adults|121660|\n",
      "|Young Adults| 33758|\n",
      "|    Children| 16014|\n",
      "|     Elderly|  6011|\n",
      "+------------+------+"
     ]
    }
   ],
   "source": [
    "# Implementation 2: Dataframe API (UDF)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "def age_group_func(age):\n",
    "    if age is None:\n",
    "        return None\n",
    "    try:\n",
    "        age = int(age)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if age < 18:\n",
    "        return \"Children\"\n",
    "    elif 18 <= age <= 24:\n",
    "        return \"Young Adults\"\n",
    "    elif 25 <= age <= 64:\n",
    "        return \"Adults\"\n",
    "    elif age > 64:\n",
    "        return \"Elderly\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "age_group = udf(age_group_func, StringType())\n",
    "\n",
    "# Filter aggravated assault incidents\n",
    "assault = Crime_df.filter(\n",
    "    F.lower(F.col(\"Crm Cd Desc\")).contains(\"aggravated assault\")\n",
    ")\n",
    "\n",
    "assault = assault.withColumn(\"Vict Age\", F.col(\"Vict Age\").cast(\"int\"))\n",
    "\n",
    "# Apply UDF to create age_group column\n",
    "assault_group = assault.withColumn(\n",
    "    \"age_group\",\n",
    "    age_group(F.col(\"Vict Age\"))\n",
    ")\n",
    "\n",
    "query_1 = (\n",
    "    assault_group\n",
    "    .groupBy(\"age_group\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = query_1.collect() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nExecution Time: {end_time - start_time:.4f} seconds\\n\")\n",
    "\n",
    "# Display result\n",
    "query_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ec33b4-e2d9-4574-8769-35596a9f3b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fcc596399542bb8d57bc574d15de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Time: 22.8356 seconds\n",
      "\n",
      "Adults           121660\n",
      "Young Adults     33758\n",
      "Children         16014\n",
      "Elderly          6011"
     ]
    }
   ],
   "source": [
    "# Implementation 3: RDD API\n",
    "\n",
    "import time\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Convert the relevant columns to RDD\n",
    "crime_rdd = Crime_df.select(\"Crm Cd Desc\", \"Vict Age\").rdd\n",
    "\n",
    "def map_to_age_group(row):\n",
    "    desc = row[\"Crm Cd Desc\"]\n",
    "    age = row[\"Vict Age\"]\n",
    "\n",
    "    # Filter aggravated assaults\n",
    "    if desc is None:\n",
    "        return None\n",
    "    if \"aggravated assault\" not in desc.lower():\n",
    "        return None\n",
    "\n",
    "    # Validate age\n",
    "    if age is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        age = int(age)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Assign age group\n",
    "    if age < 18:\n",
    "        return \"Children\"\n",
    "    elif age <= 24:\n",
    "        return \"Young Adults\"\n",
    "    elif age <= 64:\n",
    "        return \"Adults\"\n",
    "    else:\n",
    "        return \"Elderly\"\n",
    "\n",
    "# RDD pipeline (map → filter → reduce)\n",
    "start_time = time.time()\n",
    "\n",
    "query_1_rdd = (\n",
    "    crime_rdd\n",
    "    .map(map_to_age_group)               # Compute age group or None\n",
    "    .filter(lambda x: x is not None)     # Keep only valid records\n",
    "    .map(lambda g: (g, 1))               # Prepare for reduction\n",
    "    .reduceByKey(lambda a, b: a + b)     # Count per group\n",
    "    .sortBy(lambda x: -x[1])             # Sort descending by count\n",
    "    .collect()                           # Force computation\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nExecution Time: {end_time - start_time:.4f} seconds\\n\")\n",
    "\n",
    "# Display results\n",
    "for group, count in query_1_rdd:\n",
    "    print(f\"{group:15}  {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
